{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "verify-header-01",
   "metadata": {},
   "source": "## 1. Verify Pre-installed Packages\n\nFirst, let's check which packages are already installed. This helps identify what needs to be installed or upgraded.\n\nThe key packages for this notebook are:\n- **langgraph**: LangChain's graph-based agent execution framework\n- **langchain-aws**: AWS integrations for LangChain (Bedrock support)\n- **langchain-mcp-adapters**: Bridges MCP tools to LangChain's tool format\n- **mcp**: The Model Context Protocol client library"
  },
  {
   "cell_type": "markdown",
   "id": "d8zev2pb9m",
   "source": "# Neo4j MCP Agent with LangGraph\n\nQuery a Neo4j graph database using natural language with **LangGraph** and **AgentCore Gateway MCP**.\n\n## Overview\n\nThis notebook demonstrates how to build an AI agent that can query a Neo4j graph database using natural language. The agent uses:\n\n- **LangGraph**: LangChain's graph-based agent framework for building complex AI workflows\n- **Model Context Protocol (MCP)**: A standard protocol for connecting AI agents to external tools\n- **Neo4j MCP Server**: Exposes Neo4j databases through MCP, enabling schema introspection and Cypher query execution\n- **Amazon Bedrock**: Provides the Claude LLM for reasoning and query generation\n\n**How it works**: When you ask a question, the agent first retrieves the database schema to understand what data exists, then generates and executes Cypher queries to answer your question. LangGraph's ReAct agent pattern handles the reasoning loop automatically.\n\n## Prerequisites\n\nBefore running this notebook, ensure you have:\n\n1. **Configured `CONFIG.txt`** with the following values:\n   - `MCP_GATEWAY_URL`: The AgentCore Gateway endpoint URL\n   - `MCP_ACCESS_TOKEN`: Your authentication token for the gateway\n   \n2. **Obtained credentials** either from:\n   - Your workshop host, or\n   - Deploying the Neo4j MCP Server yourself using [aws-starter](https://github.com/neo4j-partners/aws-starter)\n\n3. **AWS credentials** configured for Amazon Bedrock access (handled automatically in SageMaker)\n\nSee the [README](./README.md) for detailed setup instructions.",
   "metadata": {}
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import importlib.metadata\n",
    "\n",
    "packages = [\n",
    "    \"langchain\",\n",
    "    \"langchain-core\",\n",
    "    \"langgraph\",\n",
    "    \"langchain-aws\",\n",
    "    \"langchain-mcp-adapters\",\n",
    "    \"mcp\",\n",
    "    \"httpx\",\n",
    "    \"boto3\",\n",
    "]\n",
    "\n",
    "print(\"Pre-installed packages:\")\n",
    "print(\"-\" * 50)\n",
    "for pkg in packages:\n",
    "    try:\n",
    "        version = importlib.metadata.version(pkg)\n",
    "        print(f\"{pkg:30} {version}\")\n",
    "    except importlib.metadata.PackageNotFoundError:\n",
    "        print(f\"{pkg:30} NOT INSTALLED\")"
   ],
   "id": "304e9509f0c2f7db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Install packages\n%pip install -U langgraph langchain-aws langchain-mcp-adapters -q",
   "id": "bfbac5db99f904f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Imports\n\nImport the required libraries for building the agent:\n\n- **ChatBedrockConverse**: LangChain's chat model wrapper for Amazon Bedrock's Converse API\n- **load_mcp_tools**: Converts MCP tools into LangChain tool format\n- **create_react_agent**: Creates a ReAct (Reasoning + Acting) agent that can use tools\n- **streamablehttp_client**: MCP's modern HTTP transport for server communication\n- **ClientSession**: Manages the MCP protocol session lifecycle",
   "id": "847e3b4b9710aff7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "import asyncio\nimport concurrent.futures\nimport warnings\nfrom datetime import timedelta\n\nfrom langchain_aws import ChatBedrockConverse\nfrom langchain_mcp_adapters.tools import load_mcp_tools\n\n# Suppress known bug: https://github.com/langchain-ai/langgraph/issues/6404\nwarnings.filterwarnings(\"ignore\", message=\"create_react_agent has been moved\")\nfrom langgraph.prebuilt import create_react_agent\n\nfrom mcp import ClientSession\nfrom mcp.client.streamable_http import streamablehttp_client\n\nprint(\"All imports successful!\")",
   "id": "304475da2293987a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Configuration\n\nLoad the model and MCP Gateway credentials from `CONFIG.txt`. The configuration includes:\n\n- **MODEL_ID**: The Bedrock model to use for reasoning (e.g., Claude). For cross-region inference profiles (starting with `us.`), we also derive the base model ID.\n- **REGION**: AWS region for Bedrock API calls\n- **MCP_GATEWAY_URL**: The AgentCore Gateway endpoint that routes requests to the Neo4j MCP Server\n- **MCP_ACCESS_TOKEN**: Bearer token for authenticating with the gateway\n\nThe gateway URL and token are provided by your workshop host or generated when you deploy your own MCP server.",
   "id": "config-header-01"
  },
  {
   "cell_type": "code",
   "id": "config-01",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Load configuration from CONFIG.txt\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv(\"../CONFIG.txt\")\n\nMODEL_ID = os.getenv(\"MODEL_ID\")\nREGION = os.getenv(\"REGION\", \"us-west-2\")\nGATEWAY_URL = os.getenv(\"MCP_GATEWAY_URL\")\nACCESS_TOKEN = os.getenv(\"MCP_ACCESS_TOKEN\")\n\n# Derive BASE_MODEL_ID for Claude inference profiles\n# us.anthropic.claude-* -> anthropic.claude-*\nif MODEL_ID and MODEL_ID.startswith(\"us.anthropic.\"):\n    BASE_MODEL_ID = MODEL_ID.replace(\"us.anthropic.\", \"anthropic.\")\nelse:\n    BASE_MODEL_ID = None\n\nprint(f\"Model:   {MODEL_ID}\")\nif BASE_MODEL_ID:\n    print(f\"Base:    {BASE_MODEL_ID}\")\nprint(f\"Region:  {REGION}\")\n\n# Validate gateway credentials\nif not GATEWAY_URL or \"your-\" in GATEWAY_URL:\n    print(\"\\nWARNING: Set MCP_GATEWAY_URL in CONFIG.txt before running MCP cells\")\nelif not ACCESS_TOKEN or \"your-\" in ACCESS_TOKEN:\n    print(\"\\nWARNING: Set MCP_ACCESS_TOKEN in CONFIG.txt before running MCP cells\")\nelse:\n    print(f\"Gateway: {GATEWAY_URL[:50]}...\")\n    print(\"Configuration OK!\")"
  },
  {
   "cell_type": "markdown",
   "id": "prompt-header-01",
   "metadata": {},
   "source": "## 4. System Prompt\n\nDefine the system prompt that guides the agent's behavior. This prompt is critical for getting good results:\n\n**Key instructions in the prompt:**\n1. **Schema-first**: Always retrieve the schema before querying to understand available node labels and relationship types\n2. **Read-only**: Only execute read queries to prevent accidental data modification\n3. **Result formatting**: Present results in a clear, human-readable format\n4. **Error handling**: If a query returns no results, explain what was searched\n\nThe prompt also includes Cypher best practices to help the LLM generate valid queries."
  },
  {
   "cell_type": "code",
   "id": "system-prompt-01",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a helpful Neo4j database assistant with access to tools that let you query a Neo4j graph database.\n",
    "\n",
    "Your capabilities include:\n",
    "- Retrieve the database schema to understand node labels, relationship types, and properties\n",
    "- Execute read-only Cypher queries to answer questions about the data\n",
    "- Do not execute any write Cypher queries\n",
    "\n",
    "When answering questions about the database:\n",
    "1. First retrieve the schema to understand the database structure\n",
    "2. Formulate appropriate Cypher queries based on the actual schema\n",
    "3. If a query returns no results, explain what you looked for and suggest alternatives\n",
    "4. Format results in a clear, human-readable way\n",
    "5. Cite the actual data returned in your response\n",
    "\n",
    "Important Cypher notes:\n",
    "- Use MATCH patterns that align with the actual schema\n",
    "- For counting, use MATCH (n:Label) RETURN count(n)\n",
    "- For listing items, add LIMIT to avoid overwhelming results\n",
    "- Handle potential NULL values gracefully\n",
    "\n",
    "Be concise but thorough in your responses.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agent-header-01",
   "metadata": {},
   "source": "## 5. Initialize LLM\n\nCreate the LangChain chat model connected to Amazon Bedrock.\n\n**ChatBedrockConverse** uses Bedrock's Converse API, which provides a unified interface across different model providers. Key settings:\n\n- **temperature=0**: Makes responses deterministic (important for consistent Cypher generation)\n- **base_model_id**: Required for cross-region inference profiles to enable tool use"
  },
  {
   "cell_type": "code",
   "id": "create-agent-01",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Build LLM config - add base_model_id for Claude inference profiles\nllm_kwargs = {\n    \"model\": MODEL_ID,\n    \"region_name\": REGION,\n    \"temperature\": 0,\n}\n\nif BASE_MODEL_ID:\n    llm_kwargs[\"base_model_id\"] = BASE_MODEL_ID\n\nllm = ChatBedrockConverse(**llm_kwargs)\n\nprint(f\"LLM initialized with {MODEL_ID}!\")"
  },
  {
   "cell_type": "markdown",
   "id": "query-header-01",
   "metadata": {},
   "source": "## 6. Query Helper\n\nCreate helper functions that handle the async/sync complexity of MCP and LangGraph.\n\n**Why the complexity?** \n- MCP uses async I/O for network efficiency\n- Jupyter notebooks have their own event loop that can conflict with async code\n- The solution: run async code in a separate thread with its own event loop\n\n**What `query_async` does:**\n1. Opens an authenticated HTTP connection to the MCP Gateway\n2. Initializes an MCP session and loads available tools\n3. Creates a LangGraph ReAct agent with the LLM and tools\n4. Invokes the agent with your question\n5. Returns the final response\n\nThe **ReAct pattern** (Reasoning + Acting) lets the agent iteratively think, call tools, observe results, and decide next steps until it has enough information to answer."
  },
  {
   "cell_type": "code",
   "id": "query-helper-01",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "async def query_async(question: str) -> str:\n    \"\"\"Ask the agent a question about the Neo4j database.\"\"\"\n    headers = {\"Authorization\": f\"Bearer {ACCESS_TOKEN}\"}\n    \n    async with streamablehttp_client(\n        GATEWAY_URL,\n        headers,\n        timeout=timedelta(seconds=120),\n        terminate_on_close=False\n    ) as (read_stream, write_stream, _):\n        async with ClientSession(read_stream, write_stream) as session:\n            await session.initialize()\n            tools = await load_mcp_tools(session)\n            \n            agent = create_react_agent(\n                model=llm,\n                tools=tools,\n                prompt=SYSTEM_PROMPT,\n            )\n            \n            result = await agent.ainvoke({\n                \"messages\": [{\"role\": \"user\", \"content\": question}]\n            })\n            \n            messages = result.get(\"messages\", [])\n            if messages:\n                last_msg = messages[-1]\n                return getattr(last_msg, \"content\", str(last_msg))\n            return \"No response\"\n\n\ndef _run_async(coro):\n    \"\"\"Run async code in a new event loop in a separate thread.\"\"\"\n    loop = asyncio.new_event_loop()\n    try:\n        return loop.run_until_complete(coro)\n    finally:\n        loop.close()\n\n\ndef query(question: str) -> str:\n    \"\"\"Ask the agent a question about the Neo4j database.\"\"\"\n    print(\"=\" * 70)\n    print(f\"Q: {question}\")\n    print(\"=\" * 70)\n    \n    # Run in a separate thread to avoid Jupyter's event loop conflicts\n    with concurrent.futures.ThreadPoolExecutor() as executor:\n        future = executor.submit(_run_async, query_async(question))\n        answer = future.result(timeout=180)\n    \n    print(f\"\\nA: {answer}\")\n    return answer"
  },
  {
   "cell_type": "markdown",
   "id": "demo-header-01",
   "metadata": {},
   "source": "## 7. Demo Queries\n\nRun these sample queries to see the agent in action. Watch the output to observe:\n\n- **Tool calls**: Which MCP tools the agent invokes and in what order\n- **Cypher generation**: The queries the LLM creates based on the schema\n- **Result synthesis**: How raw data is transformed into natural language answers\n\nEach query demonstrates a different capability—from simple schema inspection to relationship traversal."
  },
  {
   "cell_type": "code",
   "id": "demo-01",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "_ = query(\"What is the database schema? Give me a brief summary.\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "demo-02",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "_ = query(\"How many nodes are in the database by label?\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "demo-03",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "_ = query(\"What types of relationships exist in the database?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "custom-header-01",
   "metadata": {},
   "source": "## 8. Your Queries\n\nTry your own natural language questions! Some ideas based on the manufacturing dataset:\n\n- \"What requirements does the HVB_3900 component have?\"\n- \"What defects have been detected and what are their severities?\"\n- \"Which technology domains does the R2D2 product cover?\"\n- \"What components belong to the Electric Powertrain domain?\"\n- \"What changes affect battery-related requirements?\"\n\nThe agent will figure out the right Cypher query—you don't need to know the syntax."
  },
  {
   "cell_type": "code",
   "id": "custom-01",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "_ = query(\"List 5 sample records from the most populated node type.\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "custom-02",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your custom query\n",
    "# _ = query(\"Your question here\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "sagemaker": {
   "instance_type": "ml.t3.medium",
   "kernel_name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}