{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": "# Graph-Enhanced Retrieval with VectorCypherRetriever\n\nThis is where the \"Graph\" in GraphRAG really shines. The `VectorCypherRetriever` combines vector search with custom Cypher queries, allowing you to traverse graph relationships and gather additional context beyond just the matched chunks.\n\n**Why this matters:** A single chunk might contain the answer to your question, but the manufacturing traceability graph connects that chunk to its parent requirement, the component it belongs to, and the broader product context. By using graph traversal, you can automatically include this rich context in every retrieval.\n\n**Prerequisites:** Complete [02 Embeddings](02_embeddings.ipynb) first to populate the graph with embeddings and create the vector index.\n\n**Learning Objectives:**\n- Write custom Cypher retrieval queries for manufacturing traceability\n- Use VectorCypherRetriever for graph-enhanced retrieval\n- Implement the expanded context window pattern\n- Compare standard vs. graph-enhanced retrieval"
  },
  {
   "cell_type": "markdown",
   "id": "install-header",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "First, install the required packages. This only needs to be run once per session."
   ]
  },
  {
   "cell_type": "code",
   "id": "install-deps",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Install neo4j-graphrag with Bedrock support\n",
    "%pip install \"neo4j-graphrag[bedrock] @ git+https://github.com/neo4j-partners/neo4j-graphrag-python.git@bedrock-embeddings\" python-dotenv pydantic-settings nest-asyncio -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_graphrag.retrievers import VectorRetriever, VectorCypherRetriever\n",
    "from neo4j_graphrag.generation import GraphRAG\n",
    "\n",
    "from data_utils import Neo4jConnection, get_llm, get_embedder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connect-header",
   "metadata": {},
   "source": [
    "## Connect to Neo4j\n",
    "\n",
    "Create and verify the connection to your Neo4j graph database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connect",
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j = Neo4jConnection().verify()\n",
    "driver = neo4j.driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llm-header",
   "metadata": {},
   "source": "## Initialize LLM and Embedder\n\nSet up the Large Language Model (LLM) and the embedding model from AWS Bedrock. Both are configured in `CONFIG.txt` via `MODEL_ID` and `EMBEDDING_MODEL_ID`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "llm-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM and Embedder from AWS Bedrock\n",
    "llm = get_llm()\n",
    "embedder = get_embedder()\n",
    "\n",
    "print(f\"LLM: {llm.model_id}\")\n",
    "print(f\"Embedder: {embedder.model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cypher-retriever-header",
   "metadata": {},
   "source": "## VectorCypherRetriever with Custom Query\n\nThe `VectorCypherRetriever` adds a key capability: after finding chunks via vector search, it runs a **custom Cypher query** starting from those matched chunks. This lets you:\n\n- Traverse to the parent Requirement for context\n- Follow the traceability chain to the Component and TechnologyDomain\n- Follow `NEXT_CHUNK` relationships to get surrounding context\n- Gather any other graph data connected to the matched chunks\n\nThe `node` variable in your Cypher query refers to each chunk returned by the vector search."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cypher-retriever",
   "metadata": {},
   "outputs": [],
   "source": "# Custom Cypher query that returns chunk context with requirement and component info\ncontext_query = \"\"\"\nMATCH (node)<-[:HAS_CHUNK]-(req:Requirement)\nOPTIONAL MATCH (comp:Component)-[:COMPONENT_HAS_REQ]->(req)\nOPTIONAL MATCH (prev:Chunk)-[:NEXT_CHUNK]->(node)\nOPTIONAL MATCH (node)-[:NEXT_CHUNK]->(next:Chunk)\nRETURN \n    node.text AS context,\n    req.name AS requirement,\n    comp.name AS component,\n    comp.description AS component_description,\n    node.index AS chunk_index,\n    prev.text AS previous_chunk,\n    next.text AS next_chunk\n\"\"\"\n\nvector_cypher_retriever = VectorCypherRetriever(\n    driver=driver,\n    index_name='requirement_embeddings',\n    embedder=embedder,\n    retrieval_query=context_query\n)\n\nprint(\"VectorCypherRetriever initialized!\")"
  },
  {
   "cell_type": "markdown",
   "id": "cypher-explanation",
   "metadata": {},
   "source": "**Understanding the Retrieval Query:**\n\n```cypher\nMATCH (node)<-[:HAS_CHUNK]-(req:Requirement)              -- Find the parent requirement\nOPTIONAL MATCH (comp:Component)-[:COMPONENT_HAS_REQ]->(req) -- Find the component\nOPTIONAL MATCH (prev:Chunk)-[:NEXT_CHUNK]->(node)          -- Find the previous chunk\nOPTIONAL MATCH (node)-[:NEXT_CHUNK]->(next:Chunk)          -- Find the next chunk\nRETURN ...\n```\n\nThe query returns:\n1. `node.text` - The matched chunk's text (what vector search found)\n2. `req.name` - Which requirement this chunk belongs to\n3. `comp.name` - Which component the requirement is for (e.g., HVB_3900)\n4. `comp.description` - The component description (e.g., \"High-Voltage Battery\")\n5. `prev.text` / `next.text` - Adjacent chunks for context expansion\n\n> **Why OPTIONAL MATCH?** The first and last chunks don't have previous/next neighbors. OPTIONAL MATCH returns NULL instead of failing.\n\nNow let's use this retriever in a GraphRAG pipeline:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphrag-query",
   "metadata": {},
   "outputs": [],
   "source": "# Initialize GraphRAG and Perform Search\nquery = \"What are the cooling requirements for the high-voltage battery?\"\n\nrag = GraphRAG(llm=llm, retriever=vector_cypher_retriever)\nresponse = rag.search(query, retriever_config={\"top_k\": 3}, return_context=True)\n\nprint(f\"Query: \\\"{query}\\\"\")\nprint(f\"Number of results returned: {len(response.retriever_result.items)}\\n\")\nprint(\"Answer:\")\nprint(response.answer)"
  },
  {
   "cell_type": "markdown",
   "id": "context-header",
   "metadata": {},
   "source": "## Inspecting Retrieved Context\n\nOne of the best ways to debug and improve your RAG system is to inspect what context is actually being passed to the LLM. Let's look at what the VectorCypherRetriever returned:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "view-context",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the context used in this query\n",
    "print(\"Retrieved Context:\")\n",
    "print(\"=\" * 60)\n",
    "for i, item in enumerate(response.retriever_result.items):\n",
    "    print(f\"\\n[Result {i+1}]\")\n",
    "    print(item.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-context-header",
   "metadata": {},
   "source": "## Expanded Context Window Pattern\n\nThe previous query returned separate fields for prev/current/next chunks plus component metadata. A more powerful pattern is to **concatenate them into a single expanded context string** enriched with manufacturing metadata. This gives the LLM a larger window of continuous text plus structural context to work with.\n\nThink of it like this: if your chunks are 400 characters each, this pattern gives the LLM ~1200 characters of context (prev + current + next) for each vector match, **plus** the requirement name and component information. The LLM can then answer not just \"what does the text say?\" but \"which component and requirement is this about?\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-context",
   "metadata": {},
   "outputs": [],
   "source": "# Query that combines current chunk with adjacent chunks and manufacturing context\nexpanded_context_query = \"\"\"\nMATCH (node)<-[:HAS_CHUNK]-(req:Requirement)\nOPTIONAL MATCH (comp:Component)-[:COMPONENT_HAS_REQ]->(req)\nOPTIONAL MATCH (prev:Chunk)-[:NEXT_CHUNK]->(node)\nOPTIONAL MATCH (node)-[:NEXT_CHUNK]->(next:Chunk)\nWITH node, req, comp, prev, next\nRETURN \n    'Component: ' + COALESCE(comp.name, 'N/A') + ' (' + COALESCE(comp.description, '') + ')' +\n    '\\nRequirement: ' + COALESCE(req.name, 'N/A') +\n    '\\nContent: ' + COALESCE(prev.text + ' ', '') + node.text + COALESCE(' ' + next.text, '') \n    AS expanded_context,\n    req.name AS requirement_name,\n    node.index AS center_chunk_index\n\"\"\"\n\nexpanded_retriever = VectorCypherRetriever(\n    driver=driver,\n    index_name='requirement_embeddings',\n    embedder=embedder,\n    retrieval_query=expanded_context_query\n)\n\n# Test with expanded context\nquery = \"What safety standards must the battery system comply with?\"\nrag_expanded = GraphRAG(llm=llm, retriever=expanded_retriever)\nresponse = rag_expanded.search(query, retriever_config={\"top_k\": 2}, return_context=True)\n\nprint(f\"Query: \\\"{query}\\\"\\n\")\nprint(\"Answer:\")\nprint(response.answer)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "view-expanded-context",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the expanded context\n",
    "print(\"\\nExpanded Context (includes adjacent chunks):\")\n",
    "print(\"=\" * 60)\n",
    "for i, item in enumerate(response.retriever_result.items):\n",
    "    print(f\"\\n[Result {i+1}]\")\n",
    "    print(item.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-header",
   "metadata": {},
   "source": "## Comparing Standard vs Expanded Context\n\nLet's see the difference in practice. We'll ask the same question using:\n1. **Standard VectorRetriever** - Returns only the matched chunks\n2. **VectorCypherRetriever with expanded context** - Returns matched chunks + neighbors\n\nNotice how the expanded context often produces more complete, nuanced answers because the LLM has more information to work with."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison",
   "metadata": {},
   "outputs": [],
   "source": "# Standard retriever (no graph traversal)\nstandard_retriever = VectorRetriever(\n    driver=driver,\n    index_name='requirement_embeddings',\n    embedder=embedder,\n    return_properties=['text']\n)\n\nquery = \"What are the cooling system specifications?\"\n\n# Standard retriever\nprint(\"=== Standard VectorRetriever ===\")\nrag_standard = GraphRAG(llm=llm, retriever=standard_retriever)\nresponse_standard = rag_standard.search(query, retriever_config={\"top_k\": 2})\nprint(response_standard.answer)\n\n# Expanded context retriever\nprint(\"\\n=== VectorCypherRetriever (Expanded Context) ===\")\nresponse_expanded = rag_expanded.search(query, retriever_config={\"top_k\": 2})\nprint(response_expanded.answer)"
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": "## Summary\n\nIn this notebook, you learned the most powerful retrieval pattern in GraphRAG:\n\n1. **VectorCypherRetriever** - Combines vector search with custom Cypher queries. The `node` variable in your query represents each chunk found by vector search.\n\n2. **Manufacturing traceability traversal** - From a matched chunk, you can traverse to the parent Requirement, the Component it belongs to, and the broader product context. This adds structured metadata to unstructured text search.\n\n3. **Expanded context windows** - By concatenating adjacent chunks and adding component/requirement metadata, you give the LLM more context while maintaining precise vector search. This often dramatically improves answer quality.\n\n4. **The power of graphs** - This is what separates GraphRAG from simple vector stores. The relationships in your graph (HAS_CHUNK, COMPONENT_HAS_REQ, NEXT_CHUNK) enable retrieval patterns that aren't possible with vectors alone.\n\n**Key takeaway:** Vector search finds the needle in the haystack. Graph traversal provides the manufacturing context around the needle — which component, which requirement, what adjacent specifications — so the LLM understands what it found.\n\n---\n\n**Next:** [Full-Text Search](05_fulltext_search.ipynb) to learn keyword-based search patterns, or skip ahead to [Hybrid Search](06_hybrid_search.ipynb) to combine vector and full-text search in a single pipeline.\n\nContinue to [Lab 6 - Neo4j MCP Agent](../Lab_6_Neo4j_MCP_Agent/README.md) to learn how to build agents that query the knowledge graph using the Model Context Protocol."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "neo4j.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}