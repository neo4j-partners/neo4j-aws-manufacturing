{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": "# Embeddings and Vector Search\n\nThis notebook demonstrates how to generate embeddings for text chunks and perform vector similarity search using Neo4j. Embeddings are the foundation of semantic search - the ability to find content by meaning rather than exact keyword matches.\n\n**Prerequisites:** Review [01 Data Loading](01_data_loading.ipynb) to understand the manufacturing graph structure. This notebook is self-contained and will create its own data.\n\n**Learning Objectives:**\n- Understand what embeddings are and why they matter for GraphRAG\n- Use `FixedSizeSplitter` to automatically chunk requirement text\n- Generate embeddings using AWS Bedrock (Amazon Titan)\n- Create a vector index in Neo4j\n- Perform similarity search to find relevant requirement chunks\n\n---\n\n## What are Embeddings?\n\nEmbeddings are numerical representations (vectors) of text that capture semantic meaning. The key insight is that **similar texts produce similar vectors**, enabling semantic search.\n\n```\n\"thermal management system cooling\" → [0.12, -0.45, 0.78, ...] (1024 dimensions)\n\"the cooling system must provide heat transfer\" → [0.11, -0.44, 0.77, ...] (similar vector!)\n```\n\nThis is powerful because:\n- A search for \"battery cooling requirements\" will find content about \"thermal management system\" even though those exact words don't appear\n- The embedding model understands that \"cooling\" and \"thermal management\" are semantically related\n- You don't need to anticipate every possible way an engineer might phrase their query\n\n**How similarity is measured:** We use **cosine similarity** to compare vectors. A score of 1.0 means identical direction (very similar), while 0.0 means perpendicular (unrelated). In practice, scores above 0.8 typically indicate strong semantic similarity."
  },
  {
   "cell_type": "markdown",
   "id": "install-header",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "First, install the required packages. This only needs to be run once per session."
   ]
  },
  {
   "cell_type": "code",
   "id": "install-deps",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Install neo4j-graphrag with Bedrock support\n",
    "%pip install \"neo4j-graphrag[bedrock] @ git+https://github.com/neo4j-partners/neo4j-graphrag-python.git@bedrock-embeddings\" python-dotenv pydantic-settings nest-asyncio -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_graphrag.indexes import create_vector_index\n",
    "from data_utils import Neo4jConnection, DataLoader, split_text, get_embedder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sample-data-header",
   "metadata": {},
   "source": "## Sample Data\n\nLoad requirement description text from `manufacturing_data.txt`. This contains consolidated engineering requirement descriptions for the HVB_3900 high-voltage battery component."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sample-data",
   "metadata": {},
   "outputs": [],
   "source": "# Load requirement text from file\nloader = DataLoader(\"manufacturing_data.txt\")\nSAMPLE_TEXT = loader.text\n\nmetadata = loader.get_metadata()\nprint(f\"Loaded from: {metadata['name']}\")\nprint(f\"Sample text length: {metadata['size']} characters\")"
  },
  {
   "cell_type": "markdown",
   "id": "connect-header",
   "metadata": {},
   "source": [
    "## Connect to Neo4j\n",
    "\n",
    "Create a connection using the `Neo4jConnection` utility class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connect",
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j = Neo4jConnection().verify()\n",
    "driver = neo4j.driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-header",
   "metadata": {},
   "source": "## Clear Existing Data\n\nRemove any existing nodes from previous runs."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j.clear_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "splitter-header",
   "metadata": {},
   "source": "## Split Requirement Text with FixedSizeSplitter\n\nThe `FixedSizeSplitter` from neo4j-graphrag automatically splits text into chunks of a specified size with overlap.\n\n- **chunk_size**: Maximum characters per chunk\n- **chunk_overlap**: Characters to overlap between chunks (preserves context at boundaries)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-text",
   "metadata": {},
   "outputs": [],
   "source": "# Split requirement text using the utility function (smaller chunks for demo)\nchunks_text = split_text(SAMPLE_TEXT, chunk_size=400, chunk_overlap=50)\n\nprint(f\"Split into {len(chunks_text)} chunks:\\n\")\nfor i, chunk in enumerate(chunks_text):\n    print(f\"Chunk {i}: {len(chunk)} chars\")\n    print(f\"  {chunk}\\n\")"
  },
  {
   "cell_type": "markdown",
   "id": "embedder-header",
   "metadata": {},
   "source": "## Initialize Embedder\n\nCreate an embedder using AWS Bedrock. The embedding model is configured in `CONFIG.txt` via the `EMBEDDING_MODEL_ID` setting.\n\nWe're using **Amazon Titan Text Embeddings V2**, which produces 1024-dimensional vectors. This is important because:\n- The vector index dimensions must match the embedder output\n- All chunks and queries must use the same embedding model for meaningful comparisons"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedder",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = get_embedder()\n",
    "print(f\"Embedder initialized: {embedder.model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generate-header",
   "metadata": {},
   "source": "## Generate Embeddings\n\nGenerate an embedding vector for each chunk. This calls the AWS Bedrock embedding API.\n\nEach call to `embed_query()`:\n1. Sends the text to Amazon Titan via AWS Bedrock\n2. Returns a list of 1024 floating-point numbers\n3. These numbers encode the semantic meaning of the text\n\n> **Note:** Embedding generation has a cost (typically fractions of a cent per call), but it's a one-time operation. Once stored, embeddings can be searched repeatedly without additional API calls."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-embeddings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for each chunk\n",
    "chunk_embeddings = []\n",
    "for i, text in enumerate(chunks_text):\n",
    "    embedding = embedder.embed_query(text)\n",
    "    chunk_embeddings.append({\n",
    "        \"text\": text,\n",
    "        \"index\": i,\n",
    "        \"embedding\": embedding\n",
    "    })\n",
    "    print(f\"Chunk {i}: Generated {len(embedding)}-dimensional embedding\")\n",
    "\n",
    "print(f\"\\nFirst 5 values of chunk 0's embedding: {chunk_embeddings[0]['embedding'][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "store-header",
   "metadata": {},
   "source": "## Store in Neo4j with Embeddings\n\nCreate a Requirement node and Chunk nodes, storing the embedding vector on each Chunk as a property. The Requirement is linked to its Chunks via `HAS_CHUNK` relationships, and Chunks are linked to each other via `NEXT_CHUNK`.\n\nNeo4j can store vectors (lists of floats) as node properties. This is efficient because:\n- The embedding is stored directly with the chunk text it represents\n- No need for a separate vector database\n- Graph traversals can access both text and embeddings seamlessly"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "store-chunks",
   "metadata": {},
   "outputs": [],
   "source": "def store_requirement_with_chunks(driver, req_name, chunk_data):\n    \"\"\"Store a Requirement node and Chunk nodes with embeddings.\"\"\"\n    with driver.session() as session:\n        # Create Requirement\n        result = session.run(\"\"\"\n            CREATE (r:Requirement {requirement_id: '1_1', name: $name,\n                description: 'Battery Cell and Module Design'})\n            RETURN elementId(r) as req_id\n        \"\"\", name=req_name)\n        req_id = result.single()[\"req_id\"]\n        print(f\"Created Requirement: {req_name}\")\n\n        # Create Chunks with embeddings and HAS_CHUNK relationships\n        for chunk in chunk_data:\n            session.run(\"\"\"\n                MATCH (r:Requirement) WHERE elementId(r) = $req_id\n                CREATE (c:Chunk {\n                    text: $text,\n                    index: $index,\n                    embedding: $embedding\n                })\n                CREATE (r)-[:HAS_CHUNK]->(c)\n            \"\"\", req_id=req_id, text=chunk[\"text\"],\n               index=chunk[\"index\"], embedding=chunk[\"embedding\"])\n        print(f\"Created {len(chunk_data)} Chunk nodes with embeddings\")\n\n        # Create NEXT_CHUNK relationships\n        session.run(\"\"\"\n            MATCH (r:Requirement) WHERE elementId(r) = $req_id\n            MATCH (r)-[:HAS_CHUNK]->(c:Chunk)\n            WITH c ORDER BY c.index\n            WITH collect(c) as chunks\n            UNWIND range(0, size(chunks)-2) as i\n            WITH chunks[i] as c1, chunks[i+1] as c2\n            CREATE (c1)-[:NEXT_CHUNK]->(c2)\n        \"\"\", req_id=req_id)\n        print(\"Created NEXT_CHUNK relationships\")\n\nstore_requirement_with_chunks(driver, \"Battery Cell and Module Design\", chunk_embeddings)"
  },
  {
   "cell_type": "markdown",
   "id": "index-header",
   "metadata": {},
   "source": "## Create Vector Index\n\nCreate a vector index in Neo4j for efficient similarity search. The index uses cosine similarity to compare embeddings.\n\n> **Note:** The vector dimensions must match your embedding model. Amazon Titan Text Embeddings V2 produces 1024-dimensional vectors. If you change `EMBEDDING_MODEL_ID` in `CONFIG.txt`, update the dimensions accordingly."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-index",
   "metadata": {},
   "outputs": [],
   "source": "INDEX_NAME = \"requirement_embeddings\"\n\n# Drop existing index if it exists\ntry:\n    with driver.session() as session:\n        session.run(f\"DROP INDEX {INDEX_NAME} IF EXISTS\")\n        print(f\"Dropped existing index: {INDEX_NAME}\")\nexcept Exception:\n    pass\n\n# Create new vector index (1024 dimensions for Titan V2)\ncreate_vector_index(\n    driver=driver,\n    name=INDEX_NAME,\n    label=\"Chunk\",\n    embedding_property=\"embedding\",\n    dimensions=1024,\n    similarity_fn=\"cosine\"\n)\nprint(f\"Created vector index: {INDEX_NAME}\")"
  },
  {
   "cell_type": "markdown",
   "id": "search-header",
   "metadata": {},
   "source": "## Vector Similarity Search\n\nNow we can search for chunks that are semantically similar to a query. The search process:\n\n1. **Embed the query** - Convert the search query to a vector using the same embedding model\n2. **Find similar vectors** - Use the vector index to find chunks with similar embeddings\n3. **Rank by score** - Return results ordered by cosine similarity (higher = more similar)\n\nThe Neo4j procedure `db.index.vector.queryNodes()` performs an efficient approximate nearest neighbor (ANN) search, which scales well even with millions of chunks."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "search",
   "metadata": {},
   "outputs": [],
   "source": "def vector_search(driver, embedder, query: str, top_k: int = 3):\n    \"\"\"Search for chunks similar to the query.\"\"\"\n    # Generate query embedding\n    query_embedding = embedder.embed_query(query)\n\n    with driver.session() as session:\n        result = session.run(\"\"\"\n            CALL db.index.vector.queryNodes($index_name, $top_k, $embedding)\n            YIELD node, score\n            RETURN node.text as text, node.index as idx, score\n            ORDER BY score DESC\n        \"\"\", index_name=INDEX_NAME, top_k=top_k, embedding=query_embedding)\n\n        return list(result)\n\n# Test search\nquery = \"What are the thermal management requirements?\"\nprint(f\"Query: \\\"{query}\\\"\\n\")\nprint(\"=\" * 60)\n\nresults = vector_search(driver, embedder, query)\nfor i, record in enumerate(results):\n    print(f\"\\n[{i+1}] Score: {record['score']:.4f} (Chunk {record['idx']})\")\n    print(f\"    {record['text']}\")"
  },
  {
   "cell_type": "markdown",
   "id": "compare-header",
   "metadata": {},
   "source": "## Compare Different Queries\n\nTry different queries to see how semantic search finds relevant content even with different wording.\n\nNotice how:\n- \"battery cooling system\" finds content about thermal management and coolant specifications\n- \"electrical safety requirements\" finds content about high-voltage wiring and safety monitoring\n- The embedding model understands synonyms, related concepts, and engineering context"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare",
   "metadata": {},
   "outputs": [],
   "source": "queries = [\n    \"What are the energy density specifications for battery cells?\",\n    \"How is the battery pack protected against water?\",\n    \"What safety monitoring is required for the BMS?\"\n]\n\nfor query in queries:\n    print(f\"\\nQuery: \\\"{query}\\\"\")\n    print(\"-\" * 50)\n    results = vector_search(driver, embedder, query, top_k=1)\n    if results:\n        record = results[0]\n        print(f\"Best match (score: {record['score']:.4f}):\")\n        print(f\"  {record['text']}\")"
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": "## Summary\n\nIn this notebook, you learned how to enable semantic search in Neo4j:\n\n1. **Embeddings** - Numerical vectors (1024 floats for Titan V2) that capture semantic meaning. Similar texts have similar vectors.\n\n2. **FixedSizeSplitter** - Automatic text chunking with overlap to ensure context isn't lost at chunk boundaries.\n\n3. **Vector storage** - Storing embeddings as node properties in Neo4j, keeping text and vectors together with the requirement they belong to.\n\n4. **Vector index** - A specialized index (`requirement_embeddings`) that enables fast approximate nearest neighbor search.\n\n5. **Semantic search** - Finding content by meaning rather than keywords, using `db.index.vector.queryNodes()`.\n\nAt this point, you have a working semantic search system over manufacturing requirement descriptions. In the next notebook, you'll learn to use the **VectorRetriever** class which abstracts all of this into a simple API.\n\n---\n\n**Next:** [Vector Retriever](03_vector_retriever.ipynb)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "neo4j.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}