{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": "# Vector Retriever\n\nIn the previous notebook, you performed vector search manually using Cypher queries. Now you'll use the **VectorRetriever** class from neo4j-graphrag, which abstracts away the complexity and provides a clean API for semantic search.\n\nYou'll also learn to use the **GraphRAG** class, which combines retrieval with LLM generation to build a complete question-answering pipeline.\n\n**Prerequisites:** Complete [02 Embeddings](02_embeddings.ipynb) first to populate the graph with embeddings and create the vector index.\n\n**Learning Objectives:**\n- Use VectorRetriever for semantic search\n- Inspect retrieval results and similarity scores\n- Build a GraphRAG pipeline for question answering\n- Understand how retrieved context improves LLM responses"
  },
  {
   "cell_type": "markdown",
   "id": "install-header",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "First, install the required packages. This only needs to be run once per session."
   ]
  },
  {
   "cell_type": "code",
   "id": "install-deps",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Install neo4j-graphrag with Bedrock support\n",
    "%pip install \"neo4j-graphrag[bedrock] @ git+https://github.com/neo4j-partners/neo4j-graphrag-python.git@bedrock-embeddings\" python-dotenv pydantic-settings nest-asyncio -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_graphrag.retrievers import VectorRetriever\n",
    "from neo4j_graphrag.generation import GraphRAG\n",
    "\n",
    "from data_utils import Neo4jConnection, get_llm, get_embedder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connect-header",
   "metadata": {},
   "source": [
    "## Connect to Neo4j\n",
    "\n",
    "Create and verify the connection to your Neo4j graph database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connect",
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j = Neo4jConnection().verify()\n",
    "driver = neo4j.driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llm-header",
   "metadata": {},
   "source": "## Initialize LLM and Embedder\n\nSet up the Large Language Model (LLM) and the embedding model for GraphRAG workflows. Both models are configured in `CONFIG.txt`:\n\n- **LLM**: Uses AWS Bedrock via the `MODEL_ID` setting\n- **Embedder**: Uses AWS Bedrock via the `EMBEDDING_MODEL_ID` setting"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "llm-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM and Embedder from AWS Bedrock\n",
    "llm = get_llm()\n",
    "embedder = get_embedder()\n",
    "\n",
    "print(f\"LLM: {llm.model_id}\")\n",
    "print(f\"Embedder: {embedder.model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retriever-header",
   "metadata": {},
   "source": "## Initialize Vector Retriever\n\nThe `VectorRetriever` class handles all the complexity of semantic search:\n- Automatically embeds your query using the provided embedder\n- Queries the Neo4j vector index\n- Returns results with similarity scores and content\n\nThis is much cleaner than writing manual Cypher queries for every search!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retriever-setup",
   "metadata": {},
   "outputs": [],
   "source": "# Initialize Vector Retriever\nvector_retriever = VectorRetriever(\n    driver=driver,\n    index_name='requirement_embeddings',\n    embedder=embedder,\n    return_properties=['text']\n)\n\nprint(\"Vector Retriever initialized!\")"
  },
  {
   "cell_type": "markdown",
   "id": "retriever-explanation",
   "metadata": {},
   "source": "**VectorRetriever Parameters:**\n- `driver`: The Neo4j Python driver connection\n- `index_name`: Name of the vector index to search (`requirement_embeddings`)\n- `embedder`: The embedding model to convert queries to vectors\n- `return_properties`: List of node properties to include in results (e.g., `['text']`)\n\n> **Tip:** You can add more properties to `return_properties` like `['text', 'index']` to get additional metadata about each chunk.\n\n---\n\n## Diagnostic Search\n\nBefore building the full RAG pipeline, it's useful to inspect raw retrieval results. This helps you verify:\n- The vector index is working correctly\n- The right chunks are being retrieved for your queries\n- Similarity scores are reasonable (higher is better, typically 0.7+ indicates good relevance)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vector-search",
   "metadata": {},
   "outputs": [],
   "source": "# Simple Vector Search\nquery = \"What are the thermal management requirements for the battery?\"\nresult = vector_retriever.search(query_text=query, top_k=5)\n\nprint(f\"Query: \\\"{query}\\\"\")\nprint(f\"Number of results returned: {len(result.items)}\\n\")\nfor item in result.items:\n    score = item.metadata.get('score', 'N/A')\n    node_id = item.metadata.get('id', 'N/A')\n    content_preview = str(item.content)[:100]\n    print(f\"Score: {score:.4f}, Content: {content_preview}..., id: {node_id}\")"
  },
  {
   "cell_type": "markdown",
   "id": "search-explanation",
   "metadata": {},
   "source": "**How it works:**  \n1. The example `query`, \"What are the thermal management requirements for the battery?\", is created\n2. `vector_retriever.search()` runs the query and returns the top 5 matches based on vector similarity.\n3. The results are formatted displaying:\n    * The similarity score (`Score`)\n    * A snippet of the retrieved content (`Content`)\n    * The unique identifier for each chunk (`id`)\n\nThis diagnostic helps you verify that the vector search is working and inspect the quality of the top results for your query.\n\n> **Tip:**\n> Inspecting the returned results to verify relevance can help you to adjust your chunking or embedding strategy."
  },
  {
   "cell_type": "markdown",
   "id": "graphrag-header",
   "metadata": {},
   "source": "## Graph Retrieval-Augmented Generation (GraphRAG)\n\nNow let's combine retrieval with generation. The `GraphRAG` class orchestrates a complete RAG pipeline:\n\n1. **Retrieve** - Use the VectorRetriever to find relevant chunks\n2. **Augment** - Format the retrieved chunks as context for the LLM\n3. **Generate** - Send the query + context to the LLM for a grounded answer\n\nThis is the core pattern of RAG: instead of asking the LLM to answer from its training data alone, we provide relevant context from our knowledge graph so the answer is grounded in actual data."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphrag-query",
   "metadata": {},
   "outputs": [],
   "source": "# Initialize GraphRAG and Perform Search\nquery = \"What are the thermal management requirements for the battery?\"\nrag = GraphRAG(\n    llm=llm,\n    retriever=vector_retriever\n)\nresponse = rag.search(query, retriever_config={\"top_k\": 5}, return_context=True)\n\nprint(f\"Query: \\\"{query}\\\"\")\nprint(f\"Number of results returned: {len(response.retriever_result.items)}\\n\")\nprint(\"Answer:\")\nprint(response.answer)"
  },
  {
   "cell_type": "markdown",
   "id": "graphrag-explanation",
   "metadata": {},
   "source": "**How the GraphRAG Pipeline Works:**\n\n1. **Query received**: \"What are the thermal management requirements for the battery?\"\n2. **Retrieval**: The VectorRetriever finds the top-k most similar chunks\n3. **Context formatting**: The retrieved chunks are formatted into a prompt\n4. **LLM generation**: Claude receives both the question and the context\n5. **Response**: The LLM generates an answer grounded in the retrieved data\n\n**Key parameters:**\n- `retriever_config={\"top_k\": 5}`: Retrieve 5 chunks to use as context\n- `return_context=True`: Include the retrieved chunks in the response (useful for debugging)\n\nThe answer is now **grounded** in your actual manufacturing requirement data rather than the LLM's general knowledge!"
  },
  {
   "cell_type": "markdown",
   "id": "experiment-header",
   "metadata": {},
   "source": [
    "## Try Different Queries\n",
    "\n",
    "Experiment with the vector retriever by modifying the `query`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experiment",
   "metadata": {},
   "outputs": [],
   "source": "# Try different queries\nqueries = [\n    \"What are the energy density specifications for battery cells?\",\n    \"What safety standards must the BMS comply with?\",\n    \"How is the battery pack protected against water ingress?\"\n]\n\nfor query in queries:\n    print(f\"\\nQuery: \\\"{query}\\\"\")\n    print(\"-\" * 60)\n    response = rag.search(query, retriever_config={\"top_k\": 3})\n    print(f\"Answer: {response.answer}\")"
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": "## Summary\n\nIn this notebook, you built your first complete GraphRAG pipeline:\n\n1. **VectorRetriever** - Abstracts vector search into a simple `search()` method. No more manual Cypher queries for embedding lookups.\n\n2. **Diagnostic inspection** - Viewing raw retrieval results helps debug and tune your chunking/embedding strategy.\n\n3. **GraphRAG pipeline** - Combines retrieval + LLM generation for grounded question answering. The LLM's response is based on actual data from your knowledge graph.\n\n**Current limitation:** The VectorRetriever only returns the matched chunks themselves. But what if a question requires context that spans multiple chunks, or needs related graph data like which component a requirement belongs to? In the next notebook, you'll learn to use **VectorCypherRetriever** to traverse graph relationships and include adjacent chunks and related manufacturing data for expanded context.\n\n---\n\n**Next:** [Vector Cypher Retriever](04_vector_cypher_retriever.ipynb)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "neo4j.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}